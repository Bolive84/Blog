{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Generate Test Data for Machine Learning in Python using Sklearn dataset generators: Make_Regression and Make_Classification:**\n",
    "\n",
    "Good datasets may not be easy to find, and looking for, selecting, extracting, and cleaning a real-life dataset may take more time than actually understanding the algorithm you would like to test.\n",
    "\n",
    "Scikit-learn famous standard datasets boston, diabetes, digits, linnerud, iris, wine, and breat_cancer are often sufficient to quickly illustrate the behavior of various machine learning algorithms. However, these are small 'Toy' datasets, and in some situations, you may want to have access to more flexible datasets that would fit specific machine learning test problems, and asnwer specific questions like: can your model handle noisy labels? can your model tell you which features are redundant? what happens when redundant features, noise and imbalance are all present in your dataset?\n",
    "\n",
    "And guess what? scikit-learn offers you that option too! Your best friend also includes random sample generators allowing you to build synthetic datasets with different distributions and profiles to help you experiment your classification, regression, and clustering algorithms. \n",
    "\n",
    "In this blog we will try to illustrate how make_regression and make_classification sample generators work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1: make_classification:**\n",
    "\n",
    "Make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. It introduces interdependence between these features and adds various types of further noise to the data.\n",
    "\n",
    "Here are make_classification default parameters: (n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\t\n",
    "\n",
    "The main parameters you might want to play with are the following:\n",
    "<br>n_samples : The number of samples generated in the dataset.\n",
    "<br>n_features : The total number of features generated.\n",
    "<br>n_informative : The number of informative features.\n",
    "<br>n_redundant : The number of redundant features. These features are generated as random linear combinations of the informative features.\n",
    "<br>n_repeated : The number of duplicated features, drawn randomly from the informative and the redundant features.\n",
    "<br>n_classes : The number of classes (or labels) of the classification problem.\n",
    "<br>n_clusters_per_class : The number of clusters per class.\n",
    "<br>weights : The proportions of samples assigned to each class.\n",
    "<br>class_sep : Larger values spread out the clusters/classes and make the classification task easier.\n",
    "<br>random_state : to make output reproducible.<br>\n",
    "\n",
    "In the example below, we are going to genenate a synthetic classification problem that includes 5 informative features and double-check whether Catboost classifier can spot them and evaluate their relative importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a classification task using 5 informative features. Our goal being to see if Catboost is able to spot and rank informative features\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, # generates 1000 samples\n",
    "    n_features=10, # generates 10 features\n",
    "    n_informative=5, # only 1/2 of the features will actually be useful for this classification problem\n",
    "    n_redundant=0, # none of the features will be redundant\n",
    "    n_repeated=0, # none of the features will be repeated\n",
    "    n_classes=2, # I want the generator to only create 2 classes\n",
    "    n_clusters_per_class=1, # each class will includes only 1 cluster\n",
    "    weights=None, # I want my data to be balanced\n",
    "    random_state=2 # let's make this problem reproducible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the usual train-test split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9190751\ttotal: 76.1ms\tremaining: 2.21s\n",
      "1:\tlearn: 0.9050279\ttotal: 89.2ms\tremaining: 1.25s\n",
      "2:\tlearn: 0.9209040\ttotal: 99.5ms\tremaining: 895ms\n",
      "3:\tlearn: 0.9339080\ttotal: 110ms\tremaining: 713ms\n",
      "4:\tlearn: 0.9394813\ttotal: 120ms\tremaining: 600ms\n",
      "5:\tlearn: 0.9340974\ttotal: 130ms\tremaining: 520ms\n",
      "6:\tlearn: 0.9420290\ttotal: 140ms\tremaining: 461ms\n",
      "7:\tlearn: 0.9394813\ttotal: 151ms\tremaining: 416ms\n",
      "8:\tlearn: 0.9421965\ttotal: 165ms\tremaining: 386ms\n",
      "9:\tlearn: 0.9421965\ttotal: 177ms\tremaining: 354ms\n",
      "10:\tlearn: 0.9394813\ttotal: 188ms\tremaining: 325ms\n",
      "11:\tlearn: 0.9394813\ttotal: 199ms\tremaining: 298ms\n",
      "12:\tlearn: 0.9421965\ttotal: 209ms\tremaining: 274ms\n",
      "13:\tlearn: 0.9367816\ttotal: 219ms\tremaining: 251ms\n",
      "14:\tlearn: 0.9478261\ttotal: 229ms\tremaining: 229ms\n",
      "15:\tlearn: 0.9478261\ttotal: 239ms\tremaining: 209ms\n",
      "16:\tlearn: 0.9450867\ttotal: 249ms\tremaining: 190ms\n",
      "17:\tlearn: 0.9478261\ttotal: 261ms\tremaining: 174ms\n",
      "18:\tlearn: 0.9449275\ttotal: 275ms\tremaining: 159ms\n",
      "19:\tlearn: 0.9449275\ttotal: 285ms\tremaining: 143ms\n",
      "20:\tlearn: 0.9504373\ttotal: 295ms\tremaining: 126ms\n",
      "21:\tlearn: 0.9476744\ttotal: 304ms\tremaining: 111ms\n",
      "22:\tlearn: 0.9478261\ttotal: 318ms\tremaining: 96.7ms\n",
      "23:\tlearn: 0.9561404\ttotal: 332ms\tremaining: 82.9ms\n",
      "24:\tlearn: 0.9588235\ttotal: 343ms\tremaining: 68.7ms\n",
      "25:\tlearn: 0.9588235\ttotal: 356ms\tremaining: 54.7ms\n",
      "26:\tlearn: 0.9617647\ttotal: 369ms\tremaining: 41ms\n",
      "27:\tlearn: 0.9589443\ttotal: 381ms\tremaining: 27.2ms\n",
      "28:\tlearn: 0.9589443\ttotal: 391ms\tremaining: 13.5ms\n",
      "29:\tlearn: 0.9589443\ttotal: 403ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# Import Catboost, instantiate it and fit it to generated data:\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cbc = CatBoostClassifier(iterations=30,\n",
    "    learning_rate=0.1,\n",
    "    eval_metric='Precision')\n",
    " \n",
    "cbc.fit(X_train, y_train)\n",
    "\n",
    "# Pull up feature importances:\n",
    "importance = cbc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature ranking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feature_0</td>\n",
       "      <td>0.651835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feature_1</td>\n",
       "      <td>0.954872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_2</td>\n",
       "      <td>7.824673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature_3</td>\n",
       "      <td>45.012910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feature_4</td>\n",
       "      <td>13.594171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feature_5</td>\n",
       "      <td>0.756293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feature_6</td>\n",
       "      <td>0.592782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>feature_7</td>\n",
       "      <td>0.667226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feature_8</td>\n",
       "      <td>23.489231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>feature_9</td>\n",
       "      <td>6.456008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  Feature Importance\n",
       "0  feature_0            0.651835\n",
       "1  feature_1            0.954872\n",
       "2  feature_2            7.824673\n",
       "3  feature_3           45.012910\n",
       "4  feature_4           13.594171\n",
       "5  feature_5            0.756293\n",
       "6  feature_6            0.592782\n",
       "7  feature_7            0.667226\n",
       "8  feature_8           23.489231\n",
       "9  feature_9            6.456008"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display feature importances in pandas dataframe:\n",
    "features=['feature_0','feature_1','feature_2','feature_3','feature_4','feature_5','feature_6','feature_7','feature_8','feature_9']\n",
    "relative_score = cbc.feature_importances_\n",
    "\n",
    "d={'Feature': features, \"Feature Importance\": relative_score}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "print()\n",
    "print(\"Feature ranking:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking plotted:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Return the indices and sort them.\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking plotted:\")\n",
    "\n",
    "# Plot feature importances:\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importance[indices],\n",
    "       color=\"orange\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orange bars are the feature importances. As we could expect, the plot suggests that 5 features are informative, this confirms that Catboost can evaluate the importance of features on an artificial classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2: make_regression:**\n",
    "        \n",
    "In this example, we are going to use scikit-learn's make_regression to compare the linear regression and lasso regression models coefficients, to see which of this method performs the best in terms of feature selection, using coefficients.\n",
    "\n",
    "Here are make_regression default parameters: (n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
    "\n",
    "The parameters you are most likely to use are the following:\n",
    "<br>n_samples : The number of samples generated in the dataset.\n",
    "<br>n_features : The total number of features generated. \n",
    "<br>n_informative : The number of informative features.\n",
    "<br>n_targets : The number of targets generated.\n",
    "<br>bias : The bias term in the underlying linear model.\n",
    "<br>noise : The standard deviation of the gaussian noise applied to the output.\n",
    "<br>coef : Can be set to 'True' to return the coefficients of the underlying linear model.\n",
    "<br>random_state : to make output reproducible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a regression problem using 5 informative features. Our goal being to see if Lasso regularization method is more efficient than the classic Linear Regression to extract those features which contribute the most to the model training\n",
    "\n",
    "X,y, coef = make_regression(\n",
    "        n_samples=1000, # generates 1000 samples\n",
    "        n_features=10, # generates 10 features \n",
    "        n_informative=5, # only 1/2 of the features will actually be useful for this classification problem\n",
    "        n_targets=1, # we will need only one target for this example\n",
    "        bias=0, # we do not need to introduce any bias for this case\n",
    "        noise=500, # let's introduce some noise\n",
    "        coef=True, # we will need the generator to return the coefficients of the linear model generated\n",
    "        random_state=1 # let's make the output reproducible\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_coefs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>26.746067</td>\n",
       "      <td>3.285346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.50811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.322255</td>\n",
       "      <td>12.444828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "true_coefs        0.0  26.746067   3.285346        0.0        0.0   86.50811   \n",
       "\n",
       "            feature_6  feature_7  feature_8  feature_9  \n",
       "true_coefs        0.0        0.0  93.322255  12.444828  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's show in a dataframe the true coefficients made by our make_regression generator:\n",
    "true_coefs = pd.DataFrame(coef, columns =['true_coefs'], index=['feature_0','feature_1','feature_2','feature_3','feature_4','feature_5','feature_6','feature_7','feature_8','feature_9'])\n",
    "true_coefs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the usual train-test split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Lasso:  20.28181818181818\n"
     ]
    }
   ],
   "source": [
    "# Let's find the best alpha (=regularization strength) for Lasso:\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Set up a list of Lasso alphas to check.\n",
    "best_alpha_lasso = np.linspace(0.1, 100, 100)\n",
    "\n",
    "# Cross-validate over our list of Lasso alphas.\n",
    "lasso_model = LassoCV(alphas=best_alpha_lasso, cv=5)\n",
    "\n",
    "# Fit model using best Lasso alphas.\n",
    "lasso_model = lasso_model.fit(X_train, y_train)\n",
    "lasso_optimal_alpha = lasso_model.alpha_\n",
    "\n",
    "print(\"Best alpha for Lasso: \" , lasso_optimal_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_coef</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.746067</td>\n",
       "      <td>3.285346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.508110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.322255</td>\n",
       "      <td>12.444828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_coef_linear_regression</th>\n",
       "      <td>1.866844</td>\n",
       "      <td>25.702129</td>\n",
       "      <td>-3.679188</td>\n",
       "      <td>-6.112337</td>\n",
       "      <td>34.886585</td>\n",
       "      <td>92.493703</td>\n",
       "      <td>20.896593</td>\n",
       "      <td>18.801634</td>\n",
       "      <td>43.946286</td>\n",
       "      <td>0.984736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_coef_lasso_regression</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.206847</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>14.383805</td>\n",
       "      <td>72.855281</td>\n",
       "      <td>0.078641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.938246</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  feature_0  feature_1  feature_2  feature_3  \\\n",
       "true_coef                          0.000000  26.746067   3.285346   0.000000   \n",
       "predicted_coef_linear_regression   1.866844  25.702129  -3.679188  -6.112337   \n",
       "predicted_coef_lasso_regression    0.000000  10.206847  -0.000000  -0.000000   \n",
       "\n",
       "                                  feature_4  feature_5  feature_6  feature_7  \\\n",
       "true_coef                          0.000000  86.508110   0.000000   0.000000   \n",
       "predicted_coef_linear_regression  34.886585  92.493703  20.896593  18.801634   \n",
       "predicted_coef_lasso_regression   14.383805  72.855281   0.078641   0.000000   \n",
       "\n",
       "                                  feature_8  feature_9  \n",
       "true_coef                         93.322255  12.444828  \n",
       "predicted_coef_linear_regression  43.946286   0.984736  \n",
       "predicted_coef_lasso_regression   23.938246   0.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare Lasso's coefficients to classic Linear Regression's ones:\n",
    "linreg = LinearRegression()\n",
    "lasso = Lasso(alpha=lasso_model.alpha_)\n",
    "\n",
    "models = [linreg, lasso]\n",
    "model_names = ['LinearRegression', 'Lasso']\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train),\n",
    "    \n",
    "pd.DataFrame(data=[coef, linreg.coef_, lasso.coef_], columns=['feature_0','feature_1','feature_2','feature_3','feature_4','feature_5','feature_6','feature_7','feature_8','feature_9'], index=['true_coef','predicted_coef_linear_regression', 'predicted_coef_lasso_regression'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example confirms that, as expected, by imposing a constraint on the model parameters, Lasso regression embedded method allow us to visualize which variables have non-zero regression coefficients and are consequently the most strongly associated with the response variable. Obtaining a subset of predictors will reduce complexity of our our model and prevent it from over-fitting which can result in a biased and inefficient model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "    \n",
    "Scikit_learn generators are quick and easy-to-handle methods to generate synthetic datasets that allow you to test and debug your algorithms. They can be really useful for better understanding the behavior of algorithms in response to changes in their parameters. \n",
    "Make_classification and make_regression are great tools to keep in your back pocket when you want to conduct experiments on classification, regression, or clustering algorithms. These generators let you generate case specific data and tune/control many dataset properties as varied as the number of features, the number of samples, if you would like to introduce some noise, some bias, change the degree of class separation, or the class weights if it is used for classification algorithms.\n",
    "\n",
    "We all know that finding a real dataset including specific combinations of criterias with known levels can be very difficult, so stop seraching and use scikit-learn's data generators!\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
